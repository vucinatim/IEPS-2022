{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError\n",
    "import robotparser as urobot\n",
    "\n",
    "from datetime import datetime\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "import requests\n",
    "from requests.packages import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "from entities import Site, Page, PageData, Link, FrontierEntry, Error\n",
    "import repository\n",
    "import helper_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selenium set up\n",
    "path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "WEB_DRIVER_LOCATION = os.path.join(path, \"chromedriver\") # chromedriver.exe should be placed inside crawler folder\n",
    "USER_AGENT = \"fri-wier-jernejtim\"\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(f\"user-agent={USER_AGENT}\")\n",
    "chrome_options.add_argument('--ignore-ssl-errors=yes')\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Crawler set up\n",
    "LIMIT_DOMAIN = \".gov.si\"\n",
    "# SEED_URLS = ['https://e-uprava.gov.si/?view_mode=0', 'https://e-uprava.gov.si/']\n",
    "SEED_URLS = [\"http://www.gov.si/\", \"http://www.evem.gov.si/\", \"http://e-uprava.gov.si/\", \"http://e-prostor.gov.si/\"]\n",
    "BINARY_CONTENT = ['pdf', 'doc', 'docx', 'ppt', 'pptx']\n",
    "ALLOWED_LINK_TYPES = [\n",
    "    'text/html', \n",
    "    'application/pdf',\n",
    "    'application/msword',\n",
    "    'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
    "    'application/vnd.ms-powerpoint',\n",
    "    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']\n",
    "\n",
    "NUMBER_OF_WORKERS = 3\n",
    "TIMEOUT = 5 # Default timeout if no robots.txt\n",
    "START_CLEAN = False # If set to TRUE it will clear the database and start again from seed urls\n",
    "STORE_BINARY = False # If set to TRUE it will store binary data of images and files\n",
    "RESPECT_CRAWL_DELAY = True # If set to true the crawler will respect crawl_delay from robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_code(url, html_content):\n",
    "    duplicate_url = repository.check_if_duplicate(html_content)\n",
    "    if url.split('.')[-1] in BINARY_CONTENT:\n",
    "        return 'BINARY', None\n",
    "    elif url in SEED_URLS:\n",
    "        return 'FRONTIER', None\n",
    "    elif duplicate_url:\n",
    "        return 'DUPLICATE', duplicate_url\n",
    "    else:\n",
    "        return 'HTML', None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Crawled URL 'https://e-uprava.gov.si/javne-evidence/plovila.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/javne-evidence/motorna-vozila.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/javne-evidence/prosti-termini.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/javne-evidence/druge-javne-evidence.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/javne-evidence/odtujeni-osebni-dokumenti.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/javne-evidence/listine-in-potrdila.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/pomoc-kontakt/pomoc-pri-uporabi.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/pomoc-kontakt/vprasanja-in-mnenja.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/pomoc-kontakt/vodici.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/pomoc-kontakt/predlogi-uporabnikov.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/pomoc-kontakt/izjava-o-dostopnosti.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/pomoc-kontakt/vsa-pogosta-vprasanja.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/it.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/hu.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/en.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/o-e-upravi/o-e-upravi.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/o-e-upravi/pogoji-uporabe.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/o-e-upravi/piskotki.html'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/si'\n",
      "Already Crawled URL 'https://e-uprava.gov.si/it'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32588/1342665962.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepository\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next_frontier_entry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mnext_frontier_entry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnext_frontier_entry\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\wier\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\wier\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def crawl_page(driver: WebDriver, frontier_entry, timeout):\n",
    "    frontier_id, src_url, dest_url = frontier_entry\n",
    "    o = urlparse(dest_url)\n",
    "\n",
    "    # Read and parse robots.txt file\n",
    "    # print(f\"Reading 'robots.txt' for domain '{o.hostname}'\")\n",
    "    rp, robots_content, sitemap_content = helper_functions.parse_robots_file(dest_url)\n",
    "    if RESPECT_CRAWL_DELAY:\n",
    "        crawl_delay = rp.crawl_delay(USER_AGENT)\n",
    "        if crawl_delay: timeout = crawl_delay\n",
    "\n",
    "    time.sleep(timeout)\n",
    "    # If this page was already crawled\n",
    "    # -> skip this page\n",
    "    if repository.check_if_page_exists(dest_url):\n",
    "        print(f\"Already Crawled URL '{dest_url}'\")\n",
    "        try:\n",
    "            repository.create_link(Link(src_url, dest_url))\n",
    "        except:\n",
    "            print(f\"Link not created dure to error...\")\n",
    "        return frontier_id, driver\n",
    "\n",
    "    # Add new site if on new domain\n",
    "    if not repository.check_if_site_exists(o.hostname):\n",
    "        new_site = Site(o.hostname, robots_content, sitemap_content)\n",
    "        repository.create_site(new_site)\n",
    "\n",
    "    # If robots.txt doesnt allow for crawling the page\n",
    "    # -> skip this page\n",
    "    if rp and not rp.can_fetch(USER_AGENT, dest_url):\n",
    "        print(f\"Prohibited access to URL '{dest_url}'\")\n",
    "        return frontier_id, driver\n",
    "    \n",
    "    # Try and fetch the page with the url\n",
    "    # If it fails -> skip it\n",
    "    status_code = None\n",
    "    try:\n",
    "        print(f\"Retrieving web page with URL '{dest_url}'\")\n",
    "        driver.get(dest_url)\n",
    "        status_code = requests.get(dest_url, verify=False).status_code\n",
    "        time.sleep(timeout)\n",
    "    except WebDriverException as e:\n",
    "        print(e)\n",
    "        repository.create_error(Error(dest_url, e.msg, datetime.now()))\n",
    "        return frontier_id, driver\n",
    "    except requests.ConnectionError as e:\n",
    "        print(e)\n",
    "        repository.create_error(Error(dest_url, e.strerror, datetime.now()))\n",
    "        return frontier_id, driver\n",
    "    except requests.TooManyRedirects as e:\n",
    "        print(e)\n",
    "        repository.create_error(Error(dest_url, e.strerror, datetime.now()))\n",
    "        return frontier_id, driver\n",
    "    except:\n",
    "        print(f\"Unknown error wher retrieving '{dest_url}' ... Skipping\")\n",
    "        return frontier_id, driver\n",
    "\n",
    "\n",
    "    type_code, duplicate_url = get_type_code(dest_url, driver.page_source)\n",
    "\n",
    "    if type_code == 'FRONTIER':\n",
    "        site_domain = o.hostname\n",
    "        html_content = driver.page_source\n",
    "        status_code = requests.get(dest_url, verify=False).status_code\n",
    "        accessed_time = datetime.now()\n",
    "        images = helper_functions.get_all_images(driver, dest_url, STORE_BINARY)\n",
    "        links = helper_functions.get_all_links(driver, LIMIT_DOMAIN, ALLOWED_LINK_TYPES)\n",
    "        page_data = None\n",
    "    \n",
    "    elif type_code == 'HTML':\n",
    "        site_domain = o.hostname\n",
    "        html_content = driver.page_source\n",
    "        status_code = requests.get(dest_url, verify=False).status_code\n",
    "        accessed_time = datetime.now()\n",
    "        images = helper_functions.get_all_images(driver, dest_url, STORE_BINARY)\n",
    "        links = helper_functions.get_all_links(driver, LIMIT_DOMAIN, ALLOWED_LINK_TYPES)\n",
    "        page_data = None\n",
    "\n",
    "    elif type_code == 'BINARY':\n",
    "        site_domain = o.hostname\n",
    "        html_content = None\n",
    "        status_code = requests.get(dest_url, verify=False).status_code\n",
    "        accessed_time = datetime.now()\n",
    "        images = []\n",
    "        links = []        \n",
    "        page_data = helper_functions.get_page_data(dest_url, STORE_BINARY)\n",
    "\n",
    "    elif type_code == 'DUPLICATE':\n",
    "        site_domain = o.hostname\n",
    "        html_content = None\n",
    "        status_code = requests.get(dest_url, verify=False).status_code\n",
    "        accessed_time = datetime.now()\n",
    "        images = []\n",
    "        links = []        \n",
    "        page_data = None\n",
    "        src_url = duplicate_url\n",
    "\n",
    "\n",
    "    # Create the page with gathered data \n",
    "    # and add it to pages\n",
    "    page = Page(\n",
    "        site_domain,\n",
    "        type_code, \n",
    "        dest_url, \n",
    "        html_content, \n",
    "        status_code, \n",
    "        accessed_time, \n",
    "        images, \n",
    "        page_data, \n",
    "        links\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        repository.create_page(page)\n",
    "    except:\n",
    "        print(f\"Not unique url db error '{dest_url}'\")\n",
    "        try:\n",
    "            repository.create_link(Link(src_url, dest_url))\n",
    "        except:\n",
    "            print(f\"Link not created due to error...\")\n",
    "        repository.update_frontier_entry_to_crawled(frontier_id)\n",
    "        return frontier_id, driver\n",
    "\n",
    "    \n",
    "    try:\n",
    "        if images: repository.create_images(images)\n",
    "        if page_data: repository.create_page_data(page_data)\n",
    "    except:\n",
    "        print(f\"Images of PageData not created due to error...\")\n",
    "\n",
    "    # Add the link between the two pages\n",
    "    repository.create_link(Link(src_url, dest_url))\n",
    "\n",
    "    print('\\n----------------------------------------- ')\n",
    "    print(page.__str__())\n",
    "    # print(links.__str__())\n",
    "    print('\\n')\n",
    "\n",
    "    # Add gathered links to the frontier\n",
    "    frontier_entries = [FrontierEntry(dest_url, link) for link in links]\n",
    "    repository.create_frontier_entries(frontier_entries)\n",
    "    repository.update_frontier_entry_to_crawled(frontier_id)\n",
    "\n",
    "    return frontier_id, driver\n",
    "\n",
    "if START_CLEAN:\n",
    "        repository.clear_db()\n",
    "        initial_frontier = [FrontierEntry(None, url) for url in SEED_URLS]\n",
    "        repository.create_frontier_entries(initial_frontier)\n",
    "\n",
    "\n",
    "# Multi-threaded stuff\n",
    "drivers = [webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options) for _ in range(NUMBER_OF_WORKERS)]\n",
    "for d in drivers:\n",
    "    d.implicitly_wait(TIMEOUT)\n",
    "\n",
    "def update_to_processed(f):\n",
    "    frontier_id, driver = f.result()\n",
    "\n",
    "    if frontier_id:\n",
    "        repository.update_frontier_entry_to_processed(frontier_id)\n",
    "\n",
    "    if not driver:\n",
    "            print(\"An error occured and a driver is no longer available\")\n",
    "            print(\"Adding a new driver...\")\n",
    "            driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "            driver.implicitly_wait(TIMEOUT)\n",
    "\n",
    "    drivers.append(driver)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=NUMBER_OF_WORKERS) as executor:\n",
    "    futures = []\n",
    "\n",
    "    while True:\n",
    "        f = executor.submit(repository.get_next_frontier_entry)\n",
    "        next_frontier_entry = f.result()\n",
    "\n",
    "        if next_frontier_entry is None:\n",
    "            print(\"\\n\\n CURRENT FRONTIER IS EMPTY waiting for futures to complete... \\n\\n\")\n",
    "            concurrent.futures.wait(futures)\n",
    "\n",
    "            f = executor.submit(repository.get_next_frontier_entry)\n",
    "            next_frontier_entry = f.result()\n",
    "\n",
    "            if next_frontier_entry is None:\n",
    "                print(\"\\n\\n ----- FRONTIER IS EMPTY ----- \\n\\n\")\n",
    "                executor.shutdown(wait=True)\n",
    "                break\n",
    "\n",
    "        driver = drivers.pop(0) if drivers else None\n",
    "        f = executor.submit(crawl_page, driver, next_frontier_entry, TIMEOUT)\n",
    "        f.add_done_callback(update_to_processed)\n",
    "        futures.append(f)\n",
    "\n",
    "    # for d in drivers:\n",
    "    #     d.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Multi Threaded Crawler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if START_CLEAN:\n",
    "#     repository.clear_db()\n",
    "#     initial_frontier = [FrontierEntry(None, url, False) for url in SEED_URLS.copy()]\n",
    "#     repository.create_frontier_entries(initial_frontier)\n",
    "\n",
    "# curr_domain = None # Current domain\n",
    "\n",
    "# rp = urobot.RobotFileParser() # robot file parser\n",
    "\n",
    "# driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "# # driver.implicitly_wait(TIMEOUT)\n",
    "\n",
    "# while True:\n",
    "#     # Get next entry from frontier\n",
    "#     # -> break if empty\n",
    "#     next_frontier_entry = repository.get_next_frontier_entry()\n",
    "#     if not next_frontier_entry:\n",
    "#         print(\"\\n\\n ----- FRONTIER IS EMPTY ----- \\n\\n\")\n",
    "#         break\n",
    "\n",
    "#     frontier_id, src_url, dest_url = next_frontier_entry\n",
    "#     o = urlparse(dest_url)\n",
    "\n",
    "#     # If the domain is different than from the previous url\n",
    "#     # -> Read and parse robots.txt file again\n",
    "#     if o.hostname != curr_domain:\n",
    "#         print(f\"Reading 'robots.txt' for domain '{o.hostname}'\")\n",
    "#         rp, robots_content, sitemap_content = helper_functions.parse_robots_file(dest_url)\n",
    "#         if RESPECT_CRAWL_DELAY:\n",
    "#             crawl_delay = rp.crawl_delay(USER_AGENT)\n",
    "#             if crawl_delay: TIMEOUT = crawl_delay\n",
    "#         curr_domain = o.hostname\n",
    "\n",
    "#         # Add new site if on new domain\n",
    "#         if not repository.check_if_site_exists(curr_domain):\n",
    "#             new_site = Site(o.hostname, robots_content, sitemap_content)\n",
    "#             repository.create_site(new_site)\n",
    "\n",
    "#     # If this page was already crawled\n",
    "#     # -> skip this page\n",
    "#     if repository.check_if_page_exists(dest_url):\n",
    "#         print(f\"Already Crawled URL '{dest_url}'\")\n",
    "#         repository.create_link(Link(src_url, dest_url))\n",
    "#         repository.update_frontier_entry_to_crawled(frontier_id)\n",
    "#         continue\n",
    "\n",
    "#     # If robots.txt doesnt allow for crawling the page\n",
    "#     # -> skip this page\n",
    "#     if not rp.can_fetch(USER_AGENT, dest_url):\n",
    "#         print(f\"Prohibited access to URL '{dest_url}'\")\n",
    "#         repository.update_frontier_entry_to_crawled(frontier_id)\n",
    "#         continue\n",
    "    \n",
    "#     # Try and fetch the page with the url\n",
    "#     # If it fails -> skip it\n",
    "#     try:\n",
    "#         print(f\"Retrieving web page URL '{dest_url}'\")\n",
    "#         driver.get(dest_url)\n",
    "#         time.sleep(TIMEOUT)\n",
    "#     except WebDriverException as e:\n",
    "#         print(e)\n",
    "#         repository.create_error(Error(dest_url, e, datetime.now()))\n",
    "#         repository.update_frontier_entry_to_crawled(frontier_id)\n",
    "#         continue\n",
    "\n",
    "\n",
    "#     type_code, duplicate_url = get_type_code(dest_url, driver.page_source)\n",
    "\n",
    "#     if type_code == 'FRONTIER':\n",
    "#         site_domain = curr_domain\n",
    "#         html_content = driver.page_source\n",
    "#         status_code = requests.get(dest_url, verify=False).status_code\n",
    "#         accessed_time = datetime.now()\n",
    "#         images = helper_functions.get_all_images(driver, dest_url, STORE_BINARY)\n",
    "#         links = helper_functions.get_all_links(driver, LIMIT_DOMAIN, ALLOWED_LINK_TYPES)\n",
    "#         page_data = None\n",
    "    \n",
    "#     elif type_code == 'HTML':\n",
    "#         site_domain = curr_domain\n",
    "#         html_content = driver.page_source\n",
    "#         status_code = requests.get(dest_url, verify=False).status_code\n",
    "#         accessed_time = datetime.now()\n",
    "#         images = helper_functions.get_all_images(driver, dest_url, STORE_BINARY)\n",
    "#         links = helper_functions.get_all_links(driver, LIMIT_DOMAIN, ALLOWED_LINK_TYPES)\n",
    "#         page_data = None\n",
    "\n",
    "#     elif type_code == 'BINARY':\n",
    "#         site_domain = curr_domain\n",
    "#         html_content = None\n",
    "#         status_code = requests.get(dest_url, verify=False).status_code\n",
    "#         accessed_time = datetime.now()\n",
    "#         images = []\n",
    "#         links = []        \n",
    "#         page_data = helper_functions.get_page_data(dest_url, STORE_BINARY)\n",
    "\n",
    "#     elif type_code == 'DUPLICATE':\n",
    "#         site_domain = curr_domain\n",
    "#         html_content = None\n",
    "#         status_code = requests.get(dest_url, verify=False).status_code\n",
    "#         accessed_time = datetime.now()\n",
    "#         images = []\n",
    "#         links = []        \n",
    "#         page_data = None\n",
    "#         src_url = duplicate_url\n",
    "\n",
    "\n",
    "#     # Create the page with gathered data \n",
    "#     # and add it to pages\n",
    "#     page = Page(\n",
    "#         site_domain,\n",
    "#         type_code, \n",
    "#         dest_url, \n",
    "#         html_content, \n",
    "#         status_code, \n",
    "#         accessed_time, \n",
    "#         images, \n",
    "#         page_data, \n",
    "#         links\n",
    "#     )\n",
    "\n",
    "#     repository.create_page(page)\n",
    "#     if images: repository.create_images(images)\n",
    "#     if page_data: repository.create_page_data(page_data)\n",
    "\n",
    "#     # Add the link between the two pages\n",
    "#     repository.create_link(Link(src_url, dest_url))\n",
    "\n",
    "#     print('\\n----------------------------------------- ')\n",
    "#     print(page)\n",
    "#     print(links)\n",
    "#     print('\\n')\n",
    "\n",
    "#     # Add gathered links to the frontier\n",
    "#     frontier_entries = [FrontierEntry(dest_url, link) for link in links]\n",
    "#     repository.create_frontier_entries(frontier_entries)\n",
    "#     repository.update_frontier_entry_to_crawled(frontier_id)\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "# driver.get('https://spot.gov.si/')\n",
    "\n",
    "# images = helper_functions.get_all_images(driver, 'https://spot.gov.si/')\n",
    "\n",
    "# for i in images:\n",
    "#     print(i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f4d9d126035200785166326d2ba3f914caffc731a275fb2ea3b73f35dc5abc4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('wier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
